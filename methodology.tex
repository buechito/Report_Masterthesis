\chapter{Methodology}
\label{chp:sample_chapter}

In this chapter, the methods used are first introduced and then linked, so that the algorithm for solving the problem can be understood. In a first step, the model needed to predict the thermal error is introduced. Next, the Group LASSO method is explained, which is needed to select the optimal inputs.  In order to get the best possible access to the method, it is introduced from scratch by starting from the least square problem.  
The model used for prediction contains parameters that must be well chosen. For this reason, in a third step the PSO is introduced, solving the problem in an efficient way. Finally, all methods are brought into relation with each other.


%***********************************************************************

\section{ARX Models}
\label{sec:ARX_models}


In this introduction a few terms are defined, which will be used in the following. As explained in previous chapters, temperature data measured with sensors and the thermal errors measured by an on-machine measurement cycle are required.

The temperature sensors are referred to as input in the following and are abbreviated with the letter $u$. The thermal errors are called output and are abbreviated with the letter $y$. 

\subsection{Modelling of thermal errors}
\label{sec:modelling of errors}

The task of the model is to put the inputs $u$ and outputs $y$ in relation to each other. The most basic way to do this is using linear a difference equation. Tangiral \cite{Tangiral_2014} describes such a system by saying that the output of such a system can be expressed as a weighted sum of a finite number of past inputs $u$ and outputs $y$. The time discrete equation which describes the phenomenon is introduced by Ljung \cite{Ljung_1999} and can be seen in equation \ref{eq:arx_eq}. A time discrete approach is assumed here, since data generated by sensors are used, which are subject to a sampling rate. However, this approach is not limited to time discrete systems and can of course also be applied to time continuous systems as described by Ljung \cite{Ljung_1999}.

 \begin{equation}
	y[n]+a_1y[n-1]+...+a_{n_a}y[n-n_a] = b_0 u[n]+b_1u[n-1]+...+b_{n_b} u[n-n_b]
	\label{eq:arx_eq}
\end{equation}

On the left hand side of equation \ref{eq:arx_eq}, the output can be seen. The parameters $a_1 ... a_n$ are unknown and must be calculated. On the right hand side, the input with its corresponding unknown parameters $b_0 ... b_n$ can be seen. The variable $n$ represents the current time step. The terms $n_a$ and $n_b$ stand for the number of past time steps that are taken into account when predicting the current thermal error. For each input $u$, the number of past time steps $n_b$ can have a different value, which need to be optimized. It is the same for the output $y$ and its time steps $n_a$. The idea behind this formulation becomes clearer when equation \ref{eq:arx_eq} is changed to equation \ref{eq:arx_eq_extendet}. So it is easy to understand, that the output $y$ at current time $n$ depends on the past outputs and past inputs and can therefore be calculated.

 \begin{equation}
	y[n] = -(a_1y[n-1]+...+a_{n_a}y[n-n_a]) + b_0 u[n]+b_1u[n-1]+...+b_{n_b} u[n-n_b]
	\label{eq:arx_eq_extendet}
\end{equation}

To enable a compact representation, the data are written in matrices so that the equation can be represented as a matrix product. The obtained matrices thus have the form as shown in equation \ref{eq:arx_eq_matix} and \ref{eq:thetavec} respectively.

 \begin{equation}
 	\underline{\varphi}[n] = \begin{bmatrix}
		-y[n-1]...-y[n-n_a]  u[n]...u[n-n_b]
				\end{bmatrix}^T
	\label{eq:arx_eq_matix}
\end{equation}

 \begin{equation}
	\underline{\theta} =  \begin{bmatrix}
		a_1 & a_2 & ... & a_{n_a} & b_0 & ... & b_{n_b}
				\end{bmatrix}^T
	\label{eq:thetavec}
\end{equation}

By these transformations equation \ref{eq:arx_eq_extendet} can be transformed to equation \ref{eq:matrixresult}.


 \begin{equation}
			\hat{y}[n] = \underline{\varphi}^T[n] \underline{\theta}		
	\label{eq:matrixresult}
\end{equation}

Since this depends on the time steps $n$ of the past data, but also on the parameters $\underline{\theta}$ to be calculated, it is rewritten again so that we get the final equation in \ref{eq:matrixresult_2}.

 \begin{equation}
			\hat{y}[n|\theta] = \underline{\varphi}^T[n] \underline{\theta}		
	\label{eq:matrixresult_2}
\end{equation}

Now, the general model and the idea are introduced. The question is how this can be solved in an efficient way. The interesting thing about this model structure is that the parameters $\underline{\theta}$ can be found by using the least squares method, which is introduced in the next section.

\subsection{The Least Squares Method}
\label{sec:lestsquaremethod}

The goal is to determine the parameters by means of the collected data and the introduced model. In the present case, there are more data available than are needed to determine the parameters. It is therefore a overdetermined problem which must be solved. For the linear regression model from equation \ref{eq:matrixresult}, the observations $y[n]$ and $\underline{\varphi}^T[n]$ can be stacked on top of each other to form the vector $\underline{Y}$ and the matrix $\underline{\underline{\Phi}}$, so that a notation in vector form can be realized, as shown in equation \ref{eq:LS_basic}. The structure of the $\underline{Y}$ vector and the $\underline{\underline{\Phi}}$ matrix is shown in \ref{eq:Y_Phi_matrice}. The variable $N$ stands for the number of observations needed for the model.

 \begin{equation}
			\underline{Y} = \underline{\underline{\Phi}} \, \underline{\theta}		
	\label{eq:LS_basic}
\end{equation}

 \begin{equation}
			\underline{Y} =  \begin{bmatrix}
		y(1)\\
		\vdots \\
		y(N)
				\end{bmatrix}	; 
							\underline{\underline{\Phi}} =  \begin{bmatrix}
		\underline{\varphi}^T(1)\\
		\vdots \\
		\underline{\varphi}^T(N)
				\end{bmatrix}	
	\label{eq:Y_Phi_matrice}
\end{equation}

The problem is overdetermined as mentioned above, which is why the criterion in equation \ref{eq:LS_intro} is to be solved using the least squares problem. By reshaping and taking the first derivative, this can be transformed  into the form in equation \ref{eq:LS_intro_solved}, which is a closed solution to the problem, that can be solved efficiently. Since the parameters depend on recorded data, they are subject to a random test and are therefore written with a hat.

 \begin{equation}
			\hat{\underline{\theta}}_{LS} = \mathrm{arg \underset{\theta} min} \Vert (\underline{Y}-\underline{\underline{\Phi}} \, \underline{\theta}) \Vert^2_2 	
	\label{eq:LS_intro}
\end{equation}

 \begin{equation}
			\hat{\underline{\theta}}_{LS} = (\underline{\underline{\Phi}}^T\underline{\underline{\Phi}})^{-1}\underline{\underline{\Phi}}^T\underline{Y}	
	\label{eq:LS_intro_solved}
\end{equation}

For the calculation of a first parameter set, the method introduced in equation \ref{eq:LS_intro_solved} works well, since a complete data set is available in the training phase. From this point on, the output data $y$ are no longer measured on the MT, but are predicted using equation \ref{eq:matrixresult_2} and added to the vector $\underline{\underline{Y}}$. This allows a prediction to be made in the next time step. This works well over a certain period of time. However, in order to make stable predictions over a long period of time, on-machine measurements are made to supply the model with current data. From the moment that on-machine measurements are added to the vector $\underline{\underline{Y}}$, the outputs that are predicted must be excluded, otherwise the model will fit itself. This is realized by extending the least squares problem to a weighted least squares problem. This contains a weight matrix \underline{\underline{W}}, which can be seen in equation \ref{eq:weightmatrix}. It is a diagonal matrix which contains entries $w$ which can take the values 0 or 1. 
 \begin{equation}
			\underline{\underline{W}}  =  \begin{bmatrix}
		w_{1,1} & 0 & \cdots & 0 \\
		0 & w_{2,2} & \ddots & \vdots \\
		\vdots & \ddots & \ddots & 0 \\
		0 & \cdots & 0 & w_{N,N} \\
				\end{bmatrix},
				w \in {0|1}
	\label{eq:weightmatrix}
\end{equation}

The problem definition and the calculation of the solution changes only slightly compared to the standard least squares problem. The equation for calculating the parameters additionally contains the weight matrix as shown in equation \ref{eq:w_LS_intro}.

 \begin{equation}
			\hat{\underline{\theta}}_{LS} = \mathrm{arg \underset{\theta} min} \Vert \underline{\underline{W}}^{\frac{1}{2}}(\underline{Y}-\underline{\underline{\Phi}} \, \underline{\theta}) \Vert^2_2 	
	\label{eq:w_LS_intro}
\end{equation}

 \begin{equation}
	\hat{\underline{\theta}}_{LS} = (\underline{\underline{\Phi}}^T\underline{\underline{W}} \, \underline{\underline{\Phi}})^{-1}\underline{\underline{\Phi}}^T \underline{\underline{W}} \, \underline{Y}
	\label{eq:solution_w_LS}
\end{equation}

The basis for the introduction of the Group LASSO method is set. The next section will build on the concept introduced in this section.

\section{Group LASSO Method}
\label{sec:Group_LASSO}

This chapter introduces the Group LASSO method, which was introduced by Yuan and Lin \cite{Yuan_2006} in 2006. This method will be used to make the input selection on which this work will focus. The weighted least squares problem introduced above is used to derive and understand the Group LASSO equation. Finally, a 2D example is used to make a geometric interpretation which should give the reader an understanding of the processes.

\subsection{Group LASSO Equation}
\label{sec:GroupLassoEq}

The Group LASSO method goes back to work by Bakin \cite{Bakin_1995}, who developed such a concept as early as 1995, but did not use it under that name. Under the name Group LASSO, the method first appeared in the paper by Yuan et al. \cite{Yuan_2006} in 2006. More far-reaching concepts and explanations are given by Friedman et al. \cite{Friedman2010}, who links the method with the normal LASSO method in order to make a compromise between the two methods. Based on this work, equation \ref{eq:grouplasso_original} introduces the Group LASSO method in its general form, on which the formulation used here is based.

 \begin{equation}
	\hat{\underline{\beta}}_{GL} = \mathrm{arg \underset{\beta} min} \Vert  (\underline{Y} - \sum_{l=1}^L \underline{\underline{X_l}} \, \underline{\beta_l} \Vert^2_2 + \lambda \sum_{l=1}^L \sqrt{p_l} \Vert \underline{\beta_l} \Vert_2
	\label{eq:grouplasso_original}
\end{equation}

The weighting term $\sqrt{p_l}$ is due to varying group sizes and penalizes small groups due to the root operator. With the $\Vert \cdot \Vert_2$ operator the Euclidean norm is marked. The basic shape shown is now brought to the applied form as proposed by Klingsbor et al \cite{Klingspor_2018}. The nomenclature from section \ref{sec:lestsquaremethod} is used to continue this structure, which can be seen in equation \ref{eq:grouplasso}.

 \begin{equation}
	\hat{\underline{\theta}}_{GL} = \mathrm{arg \underset{\theta} min} \Vert \underline{\underline{W}}^{\frac{1}{2}} (\underline{Y}-\underline{\underline{\Phi}} \, \underline{\theta}) \Vert^2_2 + \lambda \sum_{m=1}^p \sqrt{n^m_b} \Vert \underline{\theta}^m_B \Vert_2
	\label{eq:grouplasso}
\end{equation}

\begin{figure}[!htb]
    \centering
    \includegraphics[width=1.0\linewidth]{Inkscape/LASSO_und_LS} %NOTE that no .pdf has to be written
    \caption[2D Example Least Square vs. Gruop LASSO]{2D Exemple of a Least Square/Group LASSO Problem. On the left hand side: Least Square Problem. On the right hand side: Extension to Group LASSO problem. The grey circles in the centre of the coordinate System show the constraint of the LASSO term $\Vert \theta^m_B \Vert_2$, which is a circle in 2D space. The ellipsis on the right hand side describe the least square problem. The influence of the parameter $\lambda$ is shown from zero to infinity.}
    \label{fig:2D_LASSO}
\end{figure}

The first term corresponds to the weighted Least Squares term as introduced in equation \ref{eq:solution_w_LS} The second term is called regularization term and punishes large numbers of inputs. The parameter $\lambda$ controls the significance of the regularization term. It is very important how this $\lambda$ is selected, since it can be used to set how many inputs are selected (more on this in chapter \ref{sec:Akaike}). Furthermore, the regularization term contains the sum of a group of parameters with the corresponding time orders, which are under the root operator as explained above.

An illustrative example based on a 2D problem is shown in figure \ref{fig:2D_LASSO}. The individual terms can be interpreted geometrically. The regularization term geometrically forms circles that are at the origin. The least square problem itself forms ellipses, which are somewhere in the space. If the least square problem is considered individually, the optimum of the parameters lies in the minimum of this problem, which can be seen in figure \ref{fig:2D_LASSO}. 

From a geometric point of view, the Group LASSO problem solves the least square problem under constraints. In other words, the solution lies at the intersection of the least square problem and the constraint of the regularization term. The location of the intersection now strongly depends on how the parameter $\lambda$ is selected. If the parameter $\lambda$ is set to zero, the regularization term disappears completely and no input selection takes place. However, if the parameter $\lambda$ is chosen to be very large or even get against $\infty$, the parameter vector suddenly becomes the zero vector. How the LASSO problem can be interpreted geometrically can be seen in the right plot in figure \ref{fig:2D_LASSO}, where the extrema of the parameter $\lambda$ are shown. 

It is clear that the circles and ellipses in 3D space correspond to a sphere or an ellipsoid and have no geometrically representable shape in higher dimensions.


\begin{figure}[!htb]
    \centering
    \includegraphics[width=0.8\linewidth]{Inkscape/LASSO_parameter} %NOTE that no .pdf has to be written
    \caption[Influence of the $\lambda$ parameter]{In the figure, the parameter Lambda is plotted against the value of the different parameters. As $\lambda$ grows, fewer and fewer inputs are selected. For $\lambda = 10^-2$ 24 of 25 Inputs are selected. For $\lambda = 10^2$ only 1 of 25 Inputs is selected.
}
    \label{fig:LASSO_parameter}
\end{figure}

The influence of the parameter $\lambda$ is clearly visible in figure \ref{fig:LASSO_parameter}. If it is chosen very small, as can be seen on the left side of the plot, the regularization term has almost no influence. So all inputs are selected and the result does not differ from the least squares algorithm. With increasing parameter $\lambda$ the number of selected inputs decreases until at some point no input is selected. As already mentioned, it is of great importance how the parameter $\lambda$ is selected. How the selection of the optimal $\lambda$ is done is explained in detail in chapter \ref{sec:Akaike}.

\subsection{Alternating Direction Method of Multipliers}
\label{sec:ADMM}

As already mentioned above, the posed problem cannot be solved in a closed form, so a method, which can handle this problem is needed. The Alternating direction method of multipliers (ADMM) method, introduced by Boyd et al. \cite{Boyd_2010}, has proven to be suitable for the posed problem. Code published by Boyd was also used and adapted as a basis for the solver used in this thesis. The problem to be solved has the form already introduced in equation \ref{eq:grouplasso}, which is repeated in equation \ref{eq:admm_gl}.


 \begin{equation}
	\hat{\underline{\theta}}_{GL} = \mathrm{arg \underset{\theta} min} \Vert \underline{\underline{W}}^{\frac{1}{2}} (\underline{Y}-\underline{\underline{\Phi}} \, \underline{\theta}) \Vert^2_2 + \lambda \sum_{m=1}^p \sqrt{n^m_b} \Vert \underline{\theta}^m_B \Vert_2
	\label{eq:admm_gl}
\end{equation}

In order to use the ADMM method, equation \ref{eq:admm_gl} must be slightly rewritten. For this purpose, the parameter $\underline{\theta}$, which occurs in the least square problem, but also in the regularization term, is divided into two new parameters $\underline{\theta}$ and $\underline{\varphi}$. $\underline{\theta}$ is now only included in the Least Square problem and the $\underline{\varphi}$ parameter in the regularization term. This change creates two equations with two unknowns. On the one hand the Group LASSO equation and on the other hand the boundary condition which states that the two parameters $\underline{\theta}$ and $\underline{\varphi}$ must be the same. This leads from equation \ref{eq:admm_gl} to equation \ref{eq:admm_extended}, which is shown below.

 \begin{equation}
	\mathrm{arg \underset{\theta, \varphi} min} \Vert \underline{\underline{W}}^{\frac{1}{2}} (\underline{Y}-\underline{\underline{\Phi}} \, \underline{\theta}) \Vert^2_2 + \lambda \sum_{m=1}^p \sqrt{n^m_b} \Vert \underline{\varphi}^m_B \Vert_2 \hspace{0.5cm} \mathrm{subject \: to} \hspace{0.5cm} \underline{\theta} - \underline{\varphi} = \underline{0}
	\label{eq:admm_extended}
\end{equation}

By transforming equation \ref{eq:admm_extended} and differentiating it, the following three equations are obtained, which can then be implemented. Equation \ref{eq:eq1} stands for the least square problem. This is easily recognized by the similarities to the Least Square problem of equation \ref{eq:solution_w_LS}. Equation \ref{eq:eq2} minimizes the regularization term. Equation \ref{eq:eq3} is a measure of how well the constraint is met. If this goes to zero, the value of $\underline{v}$ no longer changes and the iteration is terminated. The parameter $\rho$ is called the Lagrangian multiplier and has an analogue function as in the Lagrangian multiplier method.


\begin{align}
	\underline{\theta}^{(k)} &= (\underline{\underline{\Phi}}^T \, \underline{\underline{W}} \, \underline{\underline{\Phi}} + \rho \underline{\underline{I}})^{-1}(\underline{\underline{\Phi}}^T \, \underline{\underline{W}} \, \underline{Y} + \rho (\underline{\varphi}^{(k-1)}-\underline{v}^{(k-1)})) \label{eq:eq1} \\ 
	\underline{\varphi}^{(k)}_g &= R_{c_g \lambda/ \rho}(\underline{\theta}_g^{(k)}+\underline{v}_g^{(k-1)}), \hspace{0.5cm} g= 1,...G  \label{eq:eq2} \\ 
	\underline{v}^{(k)} &= \underline{v}^{(k-1)}+ \underline{\theta}^{(k)}-\underline{\varphi}^{(k)}
	\label{eq:eq3}
\end{align}

A so-called soft-thresholding operator, shown in equation \ref{eq:thresholdop}, is used to minimize the second equation. This has the function of setting the respective parameters for a given $\lambda$ exactly to zero. This soft-thresholding operator  is called groupwise according to the groups of the model. If the $L_2$-norm of a group is smaller than the quotient of $\lambda$ and $\rho$, the term in brackets is negative and therefore automatically set to zero. This function is identified by the small $(\cdots)_+$ at the end of the bracket.


 \begin{equation}
	R_{c_g \lambda/ \rho}(\underline{\vartheta}_g) =\biggl( 1-\frac{t}{\Vert \underline{\vartheta}_g \Vert_2}\biggr)_+  \cdot\underline{\vartheta}_g \hspace{1.5cm} t=\frac{\lambda}{\rho} \hspace{1.5cm}
	\label{eq:thresholdop}
\end{equation}

In order to get a better understanding of the methodology and the soft-thresholding operator, an example is shown below to illustrate how the problem can be interpreted using a 2D example. In figure \ref{fig:LASSO_exemple} the Group LASSO Problem can be seen, which was introduced in Section \ref{sec:GroupLassoEq}. The value of $t$ is assumed to be $1$ in this example to keep this as simple as possible. The red dotted line shows that the algorithm has its origin in the minimum of the least square problem ($\lambda$ = 0), which would be the case if the parameter $\lambda$ were assumed to be continuous. The graded circles of the regularization term should show that this is a discrete problem in the parameter $\lambda$. As $\lambda$ grows, the intersection moves closer to the origin. If the $L_2$-norm of a group falls below the value 1 ($ \Vert \underline{\theta} \Vert_2 <1$), the entire group is set to zero.


\begin{figure}[!htb]
    \centering
    \includegraphics[width=0.75\linewidth]{Inkscape/LASSO_exemple} %NOTE that no .pdf has to be written
    \caption[Example of 2D Group LASSO Problem]{This example shows how the ADMM method works in principle. If the amount of a parameter (in this case $\underline{\theta}_1$) falls below one, the soft-thresholding operator $R_t$ becomes negative and the parameter is set to zero (t=1). Nice to see in the plot that the parameter $\underline{\theta}_1$ is set to zero when the value falls below this value. 
}
    \label{fig:LASSO_exemple}
\end{figure}

The ADMM problem is the current state of research and is being expanded and adapted in work by Hong et al. \cite{Hong_2018},
Deng et al. \cite{Deng_2016}, Iutzeler et al. \cite{Iutzeler_2013} and Nishihara et al. \cite{Nishihara2015}, which is not discussed in detail in this thesis.


\section{Akaike Information Criterion}
\label{sec:Akaike}

To find the optimal $\lambda$, the AIC is used. A $\lambda$ which prefers a lot of inputs is penalised because the chance of overfitting the model is relatively high. So a minimum number of inputs are required, which fit the model relatively well. The general form of the A can be described as follows. \cite{Akaike_1974}:

 \begin{equation}
	AIC = 2 \cdot k-ln(\hat{L})
	\label{eq:aic_general}
\end{equation}
	
Let $k$ be the number of model parameters and $\hat{L}$ be the likelihood value of the model. This formulation of the AIC value does not work well for the application of the underlying problem, since too many inputs are selected, which is undesirable. 

For this reason, the AIC proposed by Burnham et al. \cite{Burnham_2004} is used, which is adapted to Least Squares problems and is shown in equation \ref{eq:aic_rms}.  

 \begin{equation}
	AIC = 2 \cdot k+n \cdot ln(RSS)
	\label{eq:aic_rms}
\end{equation}

where RSS is the residual sum of squares. However, even with this assumption, too many inputs are still selected, so another penalty term is added to this equation for the number of inputs. In equation \ref{eq:aic_rms_adapt} this adaptation can be seen from the parameter $\sigma$. This parameter describes the size of the Vector, that contains the past time steps $n_b$. As a result, models that use many time steps into the past are penalized.

 \begin{equation}
	AIC = 2 \cdot k \cdot \sigma + n \cdot ln(RSS)
	\label{eq:aic_rms_adapt}
\end{equation}

Since the number of model parameters depends on the order as well as on the number of inputs, this extension is useful. The goal to obtain robust models with few inputs can be achieved.

\section{Order Optimization}
\label{sec:order_opt}

The choice of the orders $n_a$ and $n_b$ is essential for good compensation models. With the previous TCK method this problem does not exist because it is a serial process. This means that the input selection has already been made by the time the orders are selected. Since there are at most six inputs available at this time, the search space for an optimal solution is relatively small. Here is a small example to illustrate. If the maximum number of inputs is selected during the input selection, i.e. $\Omega = 6$ where $\Omega$ stands for the number of sensors, and the order of each of these inputs can be between 1 and 10, this results in a solution space of $10 ^ 6$ possible solutions, which is relatively little.\\

The great aspect about the Group LASSO method is that the input selection and the calculation of the model parameters happens in one step. The problem is that all 25 inputs are included in the calculation, which means that the solution space is huge. It is therefore not possible to try out the various options, which means that a methodology is required that enables efficient optimization.\\

In order to optimize the problem, a method is needed that can find a minimum in a solution space that has many local extrema. The most obvious method, namely the gradient descend method, is therefore out of the question, since its solution depends very much on the chosen starting point, which in this case is completely unknown. PSO proved to be a promising way of solving problems of this type, since the search area can be examined over a wide area. Consequently, the probability of getting into a local minimum is significantly smaller than with the gradient descend method. For this reason, the approach of PSO was pursued, which is explained in more detail in section \ref{sec:PSO}.


%***********************************************************************
\subsection{Particle Swarm Optimization}
\label{sec:PSO}

The PSO is a nature-analogue optimization process which looks for a solution to an optimization problem based on the biological swarm behaviour. The method was developed in 1995 by Kennedy et al. \cite{Kennedy}, and was updated in 2007 by Poli et al. \cite{Poli_2007}. The method belongs to the superordinate group of Genetic algorithms. It is a metaheuristic process and no knowledge of the search space is required. This makes the methodology interesting for problems that have many possible local solutions. \\

The idea of PSO is relatively simple. Since the method is not a gradient based method, the function to be optimized need not be differentiable. A simplified structure of the algorithm is given below.


\begin{enumerate}
\item Initialize a population array of particles with random positions and velocities on D dimensions in the search space
\item \textbf{loop}

\begin{enumerate}

\item  For each particle, evaluate the desired optimization fitness function (introduced in section \ref{sec:opt_runtime}) in D variables
\item  Compare particle's fitness evaluation with its personal best $pbest_i$. If current value is better than \textit{$pbest_i$}, then set $pbest_i$ equal to the current value, and $p_i$ equal to the current location $x_i$ in D-dimensional space.
\item Identify the particle in the neighbourhood with the best success so far, and assign its index to the variable $g$.
\item Change the velocity and position of the particle according to the fallowing equation:

 \begin{equation}
 \hspace{-2cm}
		\begin{cases} \underline{v}_{i} \longleftarrow \omega \underline{v}_i +c_k \underline{r}_1 (p_{best} -p_i)+c_s \underline{r}_2(g_{best}-p_i) \hspace{1cm} \underline{r}_1,\underline{r}_2 \in (0,1) \\
		 x_{i} \longleftarrow x_i +v_{i} \end{cases}
	\label{eq:pso_pseudo}
\end{equation}

\item If a criterion is met (usually a sufficiently good fitness or a maximum number of iterations), exit loop.

\end{enumerate}

\item \textbf{end loop}
\end{enumerate}

\begin{figure}[!htb]
    \centering
    \includegraphics[width=1.0\linewidth]{Inkscape/PSO} %NOTE that no .pdf has to be written
    \caption[Scheme of particle swarm optimization]{Schematic representation of the PSO method. The two figures differ in the parameters $\omega$, $\underline{r}_1$, $\underline{r}_2$, $c_k$ and $c_s$ . The green or blue area shows the area in which the share of the global or personal speed vector can be. From the given data of the previous iteration and the random components, the new position then results, which is therefore subject to a certain randomness.}
    \label{fig:flowchart}
\end{figure}

The parameters in the above equations can be interpreted as follows. The Parameter $\omega$ is the inertial weight, $\underline{r}_1$ and $\underline{r}_2$ are two different random vectors with entries in-between 0 and 1, $c_k$ and $c_s$ are the acceleration constants known as cognitive and social scaling parameters. Equations \ref{eq:c_parameter} and \ref{eq:chi_parameter} show how the parameters are composed. The two parameters $\phi_1$ and $\phi_2$ have the value of $2.05$, which is common in the literature.

 \begin{equation}
	\omega = \chi \hspace{1.5cm} c_k = \chi \phi_1 \hspace{1.5cm} c_s = \chi \phi_2 \hspace{1.5cm} \phi = \phi_1 + \phi_2
	\label{eq:c_parameter}
\end{equation}

 \begin{equation}
	\chi = \dfrac{2}{2-\phi-\sqrt{\phi^2-4\pi}}
	\label{eq:chi_parameter}
\end{equation}

The choice of the parameters $\omega$, $c_k$ and $c_s$ has a relatively big influence on the performance of the optimization \cite{Shi_1998,Taherkhani_2016}. In this work standard values from the literature are used for the parameters.

The method of PSO is generally defined for the number space of the real numbers $\mathbb{R}^n$. When optimizing the orders of the ARX model, however, a particle can only assume integer position ($\in \mathbb{N}^n$) values in-between 1 and 10. In this thesis , this problem is solved by rounding the values of the positions vector $p_i$ to integer values after each iteration and then using the vector obtained in this way for the next iteration. This method works well, but can still be expanded. This problem is being discussed more and more in the literature and there are some suggestions how the problem could be addressed \cite{Kennedy_1997,Lee_2008,Unler_2010}.

%***********************************************************************


\section{Sensitivity Analysis}
\label{sec:sensitivity_analysis}

To check whether the PSO method always converges against the same solution, the same experiment is performed several times. The same orders should always be chosen for the respective inputs.

The test is done with 100 random experiments, which are evaluated. The result is shown in figure \ref{sec:sensitivity_analysis}. In the middle plot the inputs are plotted on the X-axis and the random experiments on the Y-axis. With the assumed convergence the columns should have the same values, which can be seen in the plot by the grey gradations. This is not the case at first sight. But on closer inspection it can be seen, that the inputs, which are selected (3, 4 and 5), the convergence is given.  This becomes better visible by the histograms, which are shown in the lower part of the figure. In the case of no. 4 there is a 100\% match. In the case of No. 5 there is an 80\% match, with the remaining 20\% being the same input. 

In the upper plot (No. 3) no clear convergence is visible. However, it can be said that the selected order is either very small or very large. Orders in between only occur in very rare cases. 

\begin{figure}[!htb]
    \centering
    \includegraphics[width=0.8\linewidth]{inkscape/Zusammenstellung} %NOTE that no .pdf has to be written
    \caption[Sensitivity Analysis]{In the upper plot the individual inputs are plotted on the x-axis. On the y-axis 100 experiments are plotted, which have different initial conditions, because they are chosen randomly. One row therefore always stands for one experiment. The lower two plots show the histograms for the spindle behind and the C-axis.}
    \label{fig:flowchart}
\end{figure}

With this evaluation it can be shown that the chosen order of the selected inputs tends to a value. In certain cases (no. 4) this is very clear. In the case of No. 3 it is only evident that a range is preferred, which in this case are very large or very small orders. Since this is a heuristic method, this circumstance is not unexpected. To improve this behaviour, more particles could be used, which would require more computing time. On the other hand the tolerance of the abort criterion could be reduced, which would result in more iterations, which in turn would increase the computation time.

The results obtained have shown that good and robust results can be achieved with the selected settings. 



\section{Algorithm}
\label{sec:algorithm}

To realize a new approach for the adaptive input selection the previously presented methods are combined to a new algorithm. To understand how the individual methods are combined, this is shown schematically in Figure \ref{fig:flowchart}. It can be seen that the algorithm is parallelized in the $\lambda$ parameter.

The PSO is therefore carried out in parallel for different $\lambda$. The function $fun$ is called, which has the AIC value as return value. The optimization is iterated until an abort criterion is reached. This termination criterion calculates the difference between the current value and the previous value of the function. As soon as this falls below a value $\varepsilon$, the optimization is stopped. The $AIC_{min,n}$ values obtained in this way are stored in a vector for all calculated $\lambda$. In this vector, the smallest $AIC_{min}$ value with the matching $\lambda$ is searched, which is clearly linked to the vector of the optimal orders. With these values, the optimal parameters $\underline{\theta}$ can be calculated, which are required to create the prediction $E_{predict}$.




\begin{figure}[!htb]
    \centering
    \includegraphics[width=1.0\linewidth]{powertpiont/Flussdiagramm_neu_2} %NOTE that no .pdf has to be written
    \caption[Scheme of the algorithm]{Schematic representation of the algorithm. This includes all the methods used, which have been introduced above. The main program is shown on the left. The function to be optimized is shown on the right-hand side.}
    \label{fig:flowchart}
\end{figure}

%-------------------------------



 


