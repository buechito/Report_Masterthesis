\chapter{Methodology}
\label{chp:sample_chapter}


%***********************************************************************

\section{ARX Models}
\label{sec:ARX_models}


In this introduction a few terms are defined, which will be used in the following. As explained in previous chapters, temperature data measured with sensors and the thermal errors measured by an on-machine measurement cycle are required.

The temperature sensors are referred to as input in the following and are abbreviated with the letter $u$. The thermal errors are called output and are abbreviated with the letter $y$. 

\subsection{Modelling of thermal errors}
\label{sec:modelling of errors}

Thermal faults are measured at a wide variety of locations throughout the machine. 

(Picture of the machine with all sensors)

Explanation of the thermal errors. Explanation of what the influence of the cutting fluid is and how it can be recognized. As soon as the cutting fluid is switched off, undercooling takes place. This is due to evaporation, which extracts heat from the material. This undercooling can be seen in the plots by the kink that can be seen after the cutting fluid is switched off.


 \begin{equation}
	y[n]+a_1y[n-1]+...+a_{n_a}y[n-n_a] = b_0 u[n]+b_1u[n-1]+...+b_{n_b} u[n-n_b]
	\label{eq:arx_eq}
\end{equation}



 \begin{equation}
	y[n] = -(a_1y[n-1]+...+a_{n_a}y[n-n_a]) + b_0 u[n]+b_1u[n-1]+...+b_{n_b} u[n-n_b]
	\label{eq:arx_eq_extendet}
\end{equation}


 \begin{equation}
 	\underline{\varphi}[n] = \begin{bmatrix}
		-y[n-1]...-y[n-n_a]  u[n]...u[n-n_b]
				\end{bmatrix}^T
	\label{eq:arx_eq_matix}
\end{equation}

 \begin{equation}
	\underline{\theta} =  \begin{bmatrix}
		a_1 & a_2 & ... & a_{n_a} & b_0 & ... & b_{n_b}
				\end{bmatrix}^T
	\label{eq:thetavec}
\end{equation}

By these transformations equation \ref{eq:arx_eq_extendet} can be transformed to equation \ref{eq:matrixresult}.


 \begin{equation}
			\hat{y}[n] = \underline{\varphi}^T[n] \underline{\theta}		
	\label{eq:matrixresult}
\end{equation}



 \begin{equation}
			\hat{y}[n|\theta] = \underline{\varphi}^T[n] \underline{\theta}		
	\label{eq:matrixresult_2}
\end{equation}



\subsection{The Least Squares Method}
\label{sec:lestsquaremethod}



 \begin{equation}
			\underline{Y} = \underline{\underline{\Phi}} \, \underline{\theta}		
	\label{eq:LS_basic}
\end{equation}

 \begin{equation}
			\underline{Y} =  \begin{bmatrix}
		y(1)\\
		\vdots \\
		y(N)
				\end{bmatrix}	; 
							\underline{\underline{\Phi}} =  \begin{bmatrix}
		\underline{\varphi}^T(1)\\
		\vdots \\
		\underline{\varphi}^T(N)
				\end{bmatrix}	
	\label{eq:Y_Phi_matrice}
\end{equation}



 \begin{equation}
			\hat{\underline{\theta}}_{LS} = \mathrm{arg \underset{\theta} min} \Vert (\underline{Y}-\underline{\underline{\Phi}} \, \underline{\theta}) \Vert^2_2 	
	\label{eq:LS_intro}
\end{equation}

 \begin{equation}
			\hat{\underline{\theta}}_{LS} = (\underline{\underline{\Phi}}^T\underline{\underline{\Phi}})^{-1}\underline{\underline{\Phi}}^T\underline{Y}	
	\label{eq:LS_intro_solved}
\end{equation}


 \begin{equation}
			\underline{\underline{W}}  =  \begin{bmatrix}
		w_{1,1} & 0 & \cdots & 0 \\
		0 & w_{2,2} & \ddots & \vdots \\
		\vdots & \ddots & \ddots & 0 \\
		0 & \cdots & 0 & w_{N,N} \\
				\end{bmatrix},
				w \in {0|1}
	\label{eq:weightmatrix}
\end{equation}



 \begin{equation}
			\hat{\underline{\theta}}_{LS} = \mathrm{arg \underset{\theta} min} \Vert \underline{\underline{W}}^{\frac{1}{2}}(\underline{Y}-\underline{\underline{\Phi}} \, \underline{\theta}) \Vert^2_2 	
	\label{eq:w_LS_intro}
\end{equation}

 \begin{equation}
	\hat{\underline{\theta}}_{LS} = (\underline{\underline{\Phi}}^T\underline{\underline{W}} \, \underline{\underline{\Phi}})^{-1}\underline{\underline{\Phi}}^T \underline{\underline{W}} \, \underline{Y}
	\label{eq:solution_w_LS}
\end{equation}

\section{Machine Learning}
\label{sec:machine_learning}

In the case of this problem set, we have the problem, that we have to treat time series. This means, that it is difficult to use classical machine learning approaches. As an example, we can not use cross validation to validate the training and test data, what makes the Problem more difficult.

Relatively small amount of Data, because it is expensive to generate data.

\section{Group LASSO Method}
\label{sec:Group_LASSO}



\subsection{Group LASSO Equation}
\label{sec:GroupLassoEq}



 \begin{equation}
	\hat{\underline{\beta}}_{GL} = \mathrm{arg \underset{\beta} min} \Vert  (\underline{Y} - \sum_{l=1}^L \underline{\underline{X_l}} \, \underline{\beta_l} \Vert^2_2 + \lambda \sum_{l=1}^L \sqrt{p_l} \Vert \underline{\beta_l} \Vert_2
	\label{eq:grouplasso_original}
\end{equation}



 \begin{equation}
	\hat{\underline{\theta}}_{GL} = \mathrm{arg \underset{\theta} min} \Vert \underline{\underline{W}}^{\frac{1}{2}} (\underline{Y}-\underline{\underline{\Phi}} \, \underline{\theta}) \Vert^2_2 + \lambda \sum_{m=1}^p \sqrt{n^m_b} \Vert \underline{\theta}^m_B \Vert_2
	\label{eq:grouplasso}
\end{equation}

\begin{figure}[!htb]
    \centering
    \includegraphics[width=1.0\linewidth]{Inkscape/LASSO_und_LS} %NOTE that no .pdf has to be written
    \caption[2D Example Least Square vs. Gruop LASSO]{2D Exemple of a Least Square/Group LASSO Problem. On the left hand side: Least Square Problem. On the right hand side: Extension to Group LASSO problem. The grey circles in the centre of the coordinate System show the constraint of the LASSO term $\Vert \theta^m_B \Vert_2$, which is a circle in 2D space. The ellipsis on the right hand side describe the least square problem. The influence of the parameter $\lambda$ is shown from zero to infinity.}
    \label{fig:2D_LASSO}
\end{figure}

\begin{figure}[!htb]
    \centering
    \includegraphics[width=0.8\linewidth]{Inkscape/LASSO_parameter} %NOTE that no .pdf has to be written
    \caption[Influence of the $\lambda$ parameter]{In the figure, the parameter Lambda is plotted against the value of the different parameters. As $\lambda$ grows, fewer and fewer inputs are selected. For $\lambda = 10^-2$ 24 of 25 Inputs are selected. For $\lambda = 10^2$ only 1 of 25 Inputs is selected.
}
    \label{fig:LASSO_parameter}
\end{figure}

\section{Adaptiv Group LASSO Method}
\label{sec:Adaptive_Group_LASSO}

\subsection{Adaptive Group LASSO Equation}
\label{sec:Adaptive_GroupLassoEq}

 \begin{equation}
	\hat{\underline{\theta}}_{GL} = \mathrm{arg \underset{\theta} min} \Vert \underline{\underline{W}}^{\frac{1}{2}} (\underline{Y}-\underline{\underline{\Phi}} \, \underline{\theta}) \Vert^2_2 + \lambda \sum_{m=1}^p \dfrac{1}{\Vert \tilde{\theta}_m \Vert^\gamma} \sqrt{n^m_b} \Vert \underline{\theta}^m_B \Vert_2
	\label{eq:adaptive_grouplasso}
\end{equation}


\subsection{Alternating Direction Method of Multipliers}
\label{sec:ADMM}




 \begin{equation}
	\hat{\underline{\theta}}_{GL} = \mathrm{arg \underset{\theta} min} \Vert \underline{\underline{W}}^{\frac{1}{2}} (\underline{Y}-\underline{\underline{\Phi}} \, \underline{\theta}) \Vert^2_2 + \lambda \sum_{m=1}^p \sqrt{n^m_b} \Vert \underline{\theta}^m_B \Vert_2
	\label{eq:admm_gl}
\end{equation}


 \begin{equation}
	\mathrm{arg \underset{\theta, \varphi} min} \Vert \underline{\underline{W}}^{\frac{1}{2}} (\underline{Y}-\underline{\underline{\Phi}} \, \underline{\theta}) \Vert^2_2 + \lambda \sum_{m=1}^p \sqrt{n^m_b} \Vert \underline{\varphi}^m_B \Vert_2 \hspace{0.5cm} \mathrm{subject \: to} \hspace{0.5cm} \underline{\theta} - \underline{\varphi} = \underline{0}
	\label{eq:admm_extended}
\end{equation}




\begin{align}
	\underline{\theta}^{(k)} &= (\underline{\underline{\Phi}}^T \, \underline{\underline{W}} \, \underline{\underline{\Phi}} + \rho \underline{\underline{I}})^{-1}(\underline{\underline{\Phi}}^T \, \underline{\underline{W}} \, \underline{Y} + \rho (\underline{\varphi}^{(k-1)}-\underline{v}^{(k-1)})) \label{eq:eq1} \\ 
	\underline{\varphi}^{(k)}_g &= R_{c_g \lambda/ \rho}(\underline{\theta}_g^{(k)}+\underline{v}_g^{(k-1)}), \hspace{0.5cm} g= 1,...G  \label{eq:eq2} \\ 
	\underline{v}^{(k)} &= \underline{v}^{(k-1)}+ \underline{\theta}^{(k)}-\underline{\varphi}^{(k)}
	\label{eq:eq3}
\end{align}


 \begin{equation}
	R_{c_g \lambda/ \rho}(\underline{\vartheta}_g) =\biggl( 1-\frac{t}{\Vert \underline{\vartheta}_g \Vert_2}\biggr)_+  \cdot\underline{\vartheta}_g \hspace{1.5cm} t=\frac{\lambda}{\rho} \hspace{1.5cm}
	\label{eq:thresholdop}
\end{equation}




\begin{figure}[!htb]
    \centering
    \includegraphics[width=0.75\linewidth]{Inkscape/LASSO_exemple} %NOTE that no .pdf has to be written
    \caption[Example of 2D Group LASSO Problem]{This example shows how the ADMM method works in principle. If the amount of a parameter (in this case $\underline{\theta}_1$) falls below one, the soft-thresholding operator $R_t$ becomes negative and the parameter is set to zero (t=1). Nice to see in the plot that the parameter $\underline{\theta}_1$ is set to zero when the value falls below this value. 
}
    \label{fig:LASSO_exemple}
\end{figure}



\section{Information Criterion}
\label{sec:Information_Criterion}

The choice of the information criterion is crucial for the quality of the compensation, as well as the stability of the algorithm. In the literature there is a large selection of information criteria, which all have their own peculiarities and favor or penalize certain properties. The methodology used showed a behavior that tends to select too many inputs. Thus, an information criterion is needed that strongly penalizes many inputs. 

The algorithm was tested using three different information criteria. These are the Akaike Information Criterion (AIC), the Bayes Information Criterion (BIC), and the Pham Information Criterion (PIC). The formula for these three information criteria are listed below.


 \begin{equation}
	AIC = 2 \cdot k-ln(\hat{L})
	\label{eq:aic_general}
\end{equation}
	


 \begin{equation}
	AIC = 2 \cdot k+n \cdot ln(RSS)
	\label{eq:aic_rms}
\end{equation}


 \begin{equation}
	AIC = 2 \cdot k \cdot \sigma + n \cdot ln(RSS)
	\label{eq:aic_rms_adapt}
\end{equation}

\begin{figure}[!htb]
    \centering
    \includegraphics[width=1.0\linewidth]{figures/Model_complexity} %NOTE that no .pdf has to be written
    \caption[2D Example Least Square vs. Gruop LASSO]{Model Complexity}
    \label{fig:model_complexity}
\end{figure}




\section{Order Optimization}
\label{sec:order_opt}



%***********************************************************************
\subsection{Optimization}
\label{sec:optimization}




\begin{enumerate}
\item Initialize a population array of particles with random positions and velocities on D dimensions in the search space
\item \textbf{loop}

\begin{enumerate}

\item  For each particle, evaluate the desired optimization fitness function (introduced in section \ref{sec:opt_runtime}) in D variables
\item  Compare particle's fitness evaluation with its personal best $pbest_i$. If current value is better than \textit{$pbest_i$}, then set $pbest_i$ equal to the current value, and $p_i$ equal to the current location $x_i$ in D-dimensional space.
\item Identify the particle in the neighbourhood with the best success so far, and assign its index to the variable $g$.
\item Change the velocity and position of the particle according to the fallowing equation:

 \begin{equation}
 \hspace{-2cm}
		\begin{cases} \underline{v}_{i} \longleftarrow \omega \underline{v}_i +c_k \underline{r}_1 (p_{best} -p_i)+c_s \underline{r}_2(g_{best}-p_i) \hspace{1cm} \underline{r}_1,\underline{r}_2 \in (0,1) \\
		 x_{i} \longleftarrow x_i +v_{i} \end{cases}
	\label{eq:pso_pseudo}
\end{equation}

\item If a criterion is met (usually a sufficiently good fitness or a maximum number of iterations), exit loop.

\end{enumerate}

\item \textbf{end loop}
\end{enumerate}

\begin{figure}[!htb]
    \centering
    \includegraphics[width=1.0\linewidth]{Inkscape/PSO} %NOTE that no .pdf has to be written
    \caption[Scheme of particle swarm optimization]{Schematic representation of the PSO method. The two figures differ in the parameters $\omega$, $\underline{r}_1$, $\underline{r}_2$, $c_k$ and $c_s$ . The green or blue area shows the area in which the share of the global or personal speed vector can be. From the given data of the previous iteration and the random components, the new position then results, which is therefore subject to a certain randomness.}
    \label{fig:flowchart}
\end{figure}


 \begin{equation}
	\omega = \chi \hspace{1.5cm} c_k = \chi \phi_1 \hspace{1.5cm} c_s = \chi \phi_2 \hspace{1.5cm} \phi = \phi_1 + \phi_2
	\label{eq:c_parameter}
\end{equation}

 \begin{equation}
	\chi = \dfrac{2}{2-\phi-\sqrt{\phi^2-4\pi}}
	\label{eq:chi_parameter}
\end{equation}


%***********************************************************************

\section{Algorithm}
\label{sec:algorithm}




%-------------------------------



 


